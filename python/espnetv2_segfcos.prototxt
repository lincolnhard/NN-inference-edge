layer {
  name: "data"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 480
      dim: 640
    }
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 1
    stride: 2
  }
}
layer {
  name: "prelu0"
  type: "PReLU"
  bottom: "conv0"
  top: "prelu0"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "prelu0"
  top: "pool1"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "prelu0"
  top: "conv2"
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 0
    kernel_size: 1
    group: 4
    stride: 1
  }
}
layer {
  name: "prelu2"
  type: "PReLU"
  bottom: "conv2"
  top: "prelu2"
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "prelu2"
  top: "conv3"
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 8
    stride: 2
    dilation: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "prelu2"
  top: "conv4"
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 2
    kernel_size: 3
    group: 8
    stride: 2
    dilation: 2
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "prelu2"
  top: "conv5"
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 3
    kernel_size: 3
    group: 8
    stride: 2
    dilation: 3
  }
}
layer {
  name: "conv6"
  type: "Convolution"
  bottom: "prelu2"
  top: "conv6"
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 4
    kernel_size: 3
    group: 8
    stride: 2
    dilation: 4
  }
}
layer {
  name: "add7"
  type: "Eltwise"
  bottom: "conv3"
  bottom: "conv4"
  top: "add7"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "add8"
  type: "Eltwise"
  bottom: "add7"
  bottom: "conv5"
  top: "add8"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "add9"
  type: "Eltwise"
  bottom: "add8"
  bottom: "conv6"
  top: "add9"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "cat10"
  type: "Concat"
  bottom: "conv3"
  bottom: "add7"
  bottom: "add8"
  bottom: "add9"
  top: "cat10"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bn10"
  type: "BatchNorm"
  bottom: "cat10"
  top: "bn10"
}
layer {
  name: "prelu10"
  type: "PReLU"
  bottom: "bn10"
  top: "prelu10"
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "prelu10"
  top: "conv11"
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 0
    kernel_size: 1
    group: 4
    stride: 1
  }
}
layer {
  name: "cat12"
  type: "Concat"
  bottom: "conv11"
  bottom: "pool1"
  top: "cat12"
  concat_param {
    axis: 1
  }
}
layer {
  name: "pool12"
  type: "Pooling"
  bottom: "data"
  top: "pool12"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "pool14"
  type: "Pooling"
  bottom: "pool12"
  top: "pool14"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv14"
  type: "Convolution"
  bottom: "pool14"
  top: "conv14"
  convolution_param {
    num_output: 3
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: "prelu14"
  type: "PReLU"
  bottom: "conv14"
  top: "prelu14"
}
layer {
  name: "conv15"
  type: "Convolution"
  bottom: "prelu14"
  top: "conv15"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: "add16"
  type: "Eltwise"
  bottom: "conv15"
  bottom: "cat12"
  top: "add16"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "prelu17"
  type: "PReLU"
  bottom: "add16"
  top: "prelu17"
}
layer {
  name: "pool18"
  type: "Pooling"
  bottom: "prelu17"
  top: "pool18"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv19"
  type: "Convolution"
  bottom: "prelu17"
  top: "conv19"
  convolution_param {
    num_output: 16
    bias_term: false
    pad: 0
    kernel_size: 1
    group: 4
    stride: 1
  }
}
layer {
  name: "prelu19"
  type: "PReLU"
  bottom: "conv19"
  top: "prelu19"
}
layer {
  name: "conv20"
  type: "Convolution"
  bottom: "prelu19"
  top: "conv20"
  convolution_param {
    num_output: 16
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 16
    stride: 2
    dilation: 1
  }
}
layer {
  name: "conv21"
  type: "Convolution"
  bottom: "prelu19"
  top: "conv21"
  convolution_param {
    num_output: 16
    bias_term: false
    pad: 2
    kernel_size: 3
    group: 16
    stride: 2
    dilation: 2
  }
}
layer {
  name: "conv22"
  type: "Convolution"
  bottom: "prelu19"
  top: "conv22"
  convolution_param {
    num_output: 16
    bias_term: false
    pad: 3
    kernel_size: 3
    group: 16
    stride: 2
    dilation: 3
  }
}
layer {
  name: "conv23"
  type: "Convolution"
  bottom: "prelu19"
  top: "conv23"
  convolution_param {
    num_output: 16
    bias_term: false
    pad: 4
    kernel_size: 3
    group: 16
    stride: 2
    dilation: 4
  }
}
layer {
  name: "add24"
  type: "Eltwise"
  bottom: "conv20"
  bottom: "conv21"
  top: "add24"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "add25"
  type: "Eltwise"
  bottom: "add24"
  bottom: "conv22"
  top: "add25"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "add26"
  type: "Eltwise"
  bottom: "add25"
  bottom: "conv23"
  top: "add26"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "cat27"
  type: "Concat"
  bottom: "conv20"
  bottom: "add24"
  bottom: "add25"
  bottom: "add26"
  top: "cat27"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bn27"
  type: "BatchNorm"
  bottom: "cat27"
  top: "bn27"
}
layer {
  name: "prelu27"
  type: "PReLU"
  bottom: "bn27"
  top: "prelu27"
}
layer {
  name: "conv28"
  type: "Convolution"
  bottom: "prelu27"
  top: "conv28"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    kernel_size: 1
    group: 4
    stride: 1
  }
}
layer {
  name: "cat29"
  type: "Concat"
  bottom: "conv28"
  bottom: "pool18"
  top: "cat29"
  concat_param {
    axis: 1
  }
}
layer {
  name: "pool29"
  type: "Pooling"
  bottom: "data"
  top: "pool29"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "pool31"
  type: "Pooling"
  bottom: "pool29"
  top: "pool31"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "pool33"
  type: "Pooling"
  bottom: "pool31"
  top: "pool33"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv32"
  type: "Convolution"
  bottom: "pool33"
  top: "conv32"
  convolution_param {
    num_output: 3
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: "prelu32"
  type: "PReLU"
  bottom: "conv32"
  top: "prelu32"
}
layer {
  name: "conv33"
  type: "Convolution"
  bottom: "prelu32"
  top: "conv33"
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: "add34"
  type: "Eltwise"
  bottom: "conv33"
  bottom: "cat29"
  top: "add34"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "prelu35"
  type: "PReLU"
  bottom: "add34"
  top: "prelu35"
}
layer {
  name: "conv36"
  type: "Convolution"
  bottom: "prelu35"
  top: "conv36"
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 0
    kernel_size: 1
    group: 4
    stride: 1
  }
}
layer {
  name: "prelu36"
  type: "PReLU"
  bottom: "conv36"
  top: "prelu36"
}
layer {
  name: "conv37"
  type: "Convolution"
  bottom: "prelu36"
  top: "conv37"
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 32
    stride: 1
    dilation: 1
  }
}
layer {
  name: "conv38"
  type: "Convolution"
  bottom: "prelu36"
  top: "conv38"
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 2
    kernel_size: 3
    group: 32
    stride: 1
    dilation: 2
  }
}
layer {
  name: "conv39"
  type: "Convolution"
  bottom: "prelu36"
  top: "conv39"
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 3
    kernel_size: 3
    group: 32
    stride: 1
    dilation: 3
  }
}
layer {
  name: "conv40"
  type: "Convolution"
  bottom: "prelu36"
  top: "conv40"
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 4
    kernel_size: 3
    group: 32
    stride: 1
    dilation: 4
  }
}
layer {
  name: "add41"
  type: "Eltwise"
  bottom: "conv37"
  bottom: "conv38"
  top: "add41"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "add42"
  type: "Eltwise"
  bottom: "add41"
  bottom: "conv39"
  top: "add42"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "add43"
  type: "Eltwise"
  bottom: "add42"
  bottom: "conv40"
  top: "add43"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "cat44"
  type: "Concat"
  bottom: "conv37"
  bottom: "add41"
  bottom: "add42"
  bottom: "add43"
  top: "cat44"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bn44"
  type: "BatchNorm"
  bottom: "cat44"
  top: "bn44"
}
layer {
  name: "prelu44"
  type: "PReLU"
  bottom: "bn44"
  top: "prelu44"
}
layer {
  name: "conv45"
  type: "Convolution"
  bottom: "prelu44"
  top: "conv45"
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    group: 4
    stride: 1
  }
}
layer {
  name: "add46"
  type: "Eltwise"
  bottom: "prelu35"
  bottom: "conv45"
  top: "add46"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "prelu46"
  type: "PReLU"
  bottom: "add46"
  top: "prelu46"
}
layer {
  name: "conv47"
  type: "Convolution"
  bottom: "prelu46"
  top: "conv47"
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 0
    kernel_size: 1
    group: 4
    stride: 1
  }
}
layer {
  name: "prelu47"
  type: "PReLU"
  bottom: "conv47"
  top: "prelu47"
}
layer {
  name: "conv48"
  type: "Convolution"
  bottom: "prelu47"
  top: "conv48"
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 32
    stride: 1
    dilation: 1
  }
}
layer {
  name: "conv49"
  type: "Convolution"
  bottom: "prelu47"
  top: "conv49"
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 2
    kernel_size: 3
    group: 32
    stride: 1
    dilation: 2
  }
}
layer {
  name: "conv50"
  type: "Convolution"
  bottom: "prelu47"
  top: "conv50"
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 3
    kernel_size: 3
    group: 32
    stride: 1
    dilation: 3
  }
}
layer {
  name: "conv51"
  type: "Convolution"
  bottom: "prelu47"
  top: "conv51"
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 4
    kernel_size: 3
    group: 32
    stride: 1
    dilation: 4
  }
}
layer {
  name: "add52"
  type: "Eltwise"
  bottom: "conv48"
  bottom: "conv49"
  top: "add52"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "add53"
  type: "Eltwise"
  bottom: "add52"
  bottom: "conv50"
  top: "add53"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "add54"
  type: "Eltwise"
  bottom: "add53"
  bottom: "conv51"
  top: "add54"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "cat55"
  type: "Concat"
  bottom: "conv48"
  bottom: "add52"
  bottom: "add53"
  bottom: "add54"
  top: "cat55"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bn55"
  type: "BatchNorm"
  bottom: "cat55"
  top: "bn55"
}
layer {
  name: "prelu55"
  type: "PReLU"
  bottom: "bn55"
  top: "prelu55"
}
layer {
  name: "conv56"
  type: "Convolution"
  bottom: "prelu55"
  top: "conv56"
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    group: 4
    stride: 1
  }
}
layer {
  name: "add57"
  type: "Eltwise"
  bottom: "prelu46"
  bottom: "conv56"
  top: "add57"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "prelu57"
  type: "PReLU"
  bottom: "add57"
  top: "prelu57"
}
layer {
  name: "conv58"
  type: "Convolution"
  bottom: "prelu57"
  top: "conv58"
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 0
    kernel_size: 1
    group: 4
    stride: 1
  }
}
layer {
  name: "prelu58"
  type: "PReLU"
  bottom: "conv58"
  top: "prelu58"
}
layer {
  name: "conv59"
  type: "Convolution"
  bottom: "prelu58"
  top: "conv59"
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 32
    stride: 1
    dilation: 1
  }
}
layer {
  name: "conv60"
  type: "Convolution"
  bottom: "prelu58"
  top: "conv60"
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 2
    kernel_size: 3
    group: 32
    stride: 1
    dilation: 2
  }
}
layer {
  name: "conv61"
  type: "Convolution"
  bottom: "prelu58"
  top: "conv61"
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 3
    kernel_size: 3
    group: 32
    stride: 1
    dilation: 3
  }
}
layer {
  name: "conv62"
  type: "Convolution"
  bottom: "prelu58"
  top: "conv62"
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 4
    kernel_size: 3
    group: 32
    stride: 1
    dilation: 4
  }
}
layer {
  name: "add63"
  type: "Eltwise"
  bottom: "conv59"
  bottom: "conv60"
  top: "add63"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "add64"
  type: "Eltwise"
  bottom: "add63"
  bottom: "conv61"
  top: "add64"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "add65"
  type: "Eltwise"
  bottom: "add64"
  bottom: "conv62"
  top: "add65"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "cat66"
  type: "Concat"
  bottom: "conv59"
  bottom: "add63"
  bottom: "add64"
  bottom: "add65"
  top: "cat66"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bn66"
  type: "BatchNorm"
  bottom: "cat66"
  top: "bn66"
}
layer {
  name: "prelu66"
  type: "PReLU"
  bottom: "bn66"
  top: "prelu66"
}
layer {
  name: "conv67"
  type: "Convolution"
  bottom: "prelu66"
  top: "conv67"
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    group: 4
    stride: 1
  }
}
layer {
  name: "add68"
  type: "Eltwise"
  bottom: "prelu57"
  bottom: "conv67"
  top: "add68"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "prelu68"
  type: "PReLU"
  bottom: "add68"
  top: "prelu68"
}
layer {
  name: "conv69"
  type: "Convolution"
  bottom: "prelu68"
  top: "conv69"
  convolution_param {
    num_output: 48
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: "conv69_relu"
  type: "ReLU"
  bottom: "conv69"
  top: "conv69"
}
layer {
  name: "conv70"
  type: "Convolution"
  bottom: "conv69"
  top: "conv70"
  convolution_param {
    num_output: 48
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: "conv70_relu"
  type: "ReLU"
  bottom: "conv70"
  top: "conv70"
}
layer {
  name: "conv71"
  type: "Convolution"
  bottom: "conv70"
  top: "conv71"
  convolution_param {
    num_output: 48
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: "conv71_relu"
  type: "ReLU"
  bottom: "conv71"
  top: "conv71"
}
layer {
  name: "conv72"
  type: "Convolution"
  bottom: "conv71"
  top: "conv72"
  convolution_param {
    num_output: 48
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: "conv72_relu"
  type: "ReLU"
  bottom: "conv72"
  top: "conv72"
}
layer {
  name: "conv73"
  type: "Convolution"
  bottom: "conv72"
  top: "conv73"
  convolution_param {
    num_output: 2
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: "conv74"
  type: "Convolution"
  bottom: "conv72"
  top: "conv74"
  convolution_param {
    num_output: 1
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: "conv75"
  type: "Convolution"
  bottom: "conv72"
  top: "conv75"
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: "conv76"
  type: "Convolution"
  bottom: "conv72"
  top: "conv76"
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: "cls_score"
  type: "Sigmoid"
  bottom: "conv73"
  top: "cls_score"
}
layer {
  name: "centerness"
  type: "Sigmoid"
  bottom: "conv74"
  top: "centerness"
}
layer {
  name: "occlusion"
  type: "Sigmoid"
  bottom: "conv76"
  top: "occlusion"
}
layer {
  name: "pool77"
  type: "Pooling"
  bottom: "prelu68"
  top: "pool77"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv78"
  type: "Convolution"
  bottom: "prelu68"
  top: "conv78"
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 0
    kernel_size: 1
    group: 4
    stride: 1
  }
}
layer {
  name: "prelu78"
  type: "PReLU"
  bottom: "conv78"
  top: "prelu78"
}
layer {
  name: "conv79"
  type: "Convolution"
  bottom: "prelu78"
  top: "conv79"
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 32
    stride: 2
    dilation: 1
  }
}
layer {
  name: "conv80"
  type: "Convolution"
  bottom: "prelu78"
  top: "conv80"
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 2
    kernel_size: 3
    group: 32
    stride: 2
    dilation: 2
  }
}
layer {
  name: "conv81"
  type: "Convolution"
  bottom: "prelu78"
  top: "conv81"
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 3
    kernel_size: 3
    group: 32
    stride: 2
    dilation: 3
  }
}
layer {
  name: "conv82"
  type: "Convolution"
  bottom: "prelu78"
  top: "conv82"
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 4
    kernel_size: 3
    group: 32
    stride: 2
    dilation: 4
  }
}
layer {
  name: "add83"
  type: "Eltwise"
  bottom: "conv79"
  bottom: "conv80"
  top: "add83"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "add84"
  type: "Eltwise"
  bottom: "add83"
  bottom: "conv81"
  top: "add84"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "add85"
  type: "Eltwise"
  bottom: "add84"
  bottom: "conv82"
  top: "add85"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "cat86"
  type: "Concat"
  bottom: "conv79"
  bottom: "add83"
  bottom: "add84"
  bottom: "add85"
  top: "cat86"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bn86"
  type: "BatchNorm"
  bottom: "cat86"
  top: "bn86"
}
layer {
  name: "prelu86"
  type: "PReLU"
  bottom: "bn86"
  top: "prelu86"
}
layer {
  name: "conv87"
  type: "Convolution"
  bottom: "prelu86"
  top: "conv87"
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    group: 4
    stride: 1
  }
}
layer {
  name: "cat88"
  type: "Concat"
  bottom: "conv87"
  bottom: "pool77"
  top: "cat88"
  concat_param {
    axis: 1
  }
}
layer {
  name: "pool88"
  type: "Pooling"
  bottom: "data"
  top: "pool88"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "pool90"
  type: "Pooling"
  bottom: "pool88"
  top: "pool90"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "pool92"
  type: "Pooling"
  bottom: "pool90"
  top: "pool92"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "pool94"
  type: "Pooling"
  bottom: "pool92"
  top: "pool94"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv92"
  type: "Convolution"
  bottom: "pool94"
  top: "conv92"
  convolution_param {
    num_output: 3
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: "prelu92"
  type: "PReLU"
  bottom: "conv92"
  top: "prelu92"
}
layer {
  name: "conv93"
  type: "Convolution"
  bottom: "prelu92"
  top: "conv93"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: "add94"
  type: "Eltwise"
  bottom: "conv93"
  bottom: "cat88"
  top: "add94"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "prelu95"
  type: "PReLU"
  bottom: "add94"
  top: "prelu95"
}
layer {
  name: "conv96"
  type: "Convolution"
  bottom: "prelu95"
  top: "conv96"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    kernel_size: 1
    group: 4
    stride: 1
  }
}
layer {
  name: "prelu96"
  type: "PReLU"
  bottom: "conv96"
  top: "prelu96"
}
layer {
  name: "conv97"
  type: "Convolution"
  bottom: "prelu96"
  top: "conv97"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 64
    stride: 1
    dilation: 1
  }
}
layer {
  name: "conv98"
  type: "Convolution"
  bottom: "prelu96"
  top: "conv98"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 64
    stride: 1
    dilation: 1
  }
}
layer {
  name: "conv99"
  type: "Convolution"
  bottom: "prelu96"
  top: "conv99"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 2
    kernel_size: 3
    group: 64
    stride: 1
    dilation: 2
  }
}
layer {
  name: "conv100"
  type: "Convolution"
  bottom: "prelu96"
  top: "conv100"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 3
    kernel_size: 3
    group: 64
    stride: 1
    dilation: 3
  }
}
layer {
  name: "add101"
  type: "Eltwise"
  bottom: "conv97"
  bottom: "conv98"
  top: "add101"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "add102"
  type: "Eltwise"
  bottom: "add101"
  bottom: "conv99"
  top: "add102"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "add103"
  type: "Eltwise"
  bottom: "add102"
  bottom: "conv100"
  top: "add103"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "cat104"
  type: "Concat"
  bottom: "conv97"
  bottom: "add101"
  bottom: "add102"
  bottom: "add103"
  top: "cat104"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bn104"
  type: "BatchNorm"
  bottom: "cat104"
  top: "bn104"
}
layer {
  name: "prelu104"
  type: "PReLU"
  bottom: "bn104"
  top: "prelu104"
}
layer {
  name: "conv105"
  type: "Convolution"
  bottom: "prelu104"
  top: "conv105"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    group: 4
    stride: 1
  }
}
layer {
  name: "add106"
  type: "Eltwise"
  bottom: "prelu95"
  bottom: "conv105"
  top: "add106"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "prelu106"
  type: "PReLU"
  bottom: "add106"
  top: "prelu106"
}
layer {
  name: "conv107"
  type: "Convolution"
  bottom: "prelu106"
  top: "conv107"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    kernel_size: 1
    group: 4
    stride: 1
  }
}
layer {
  name: "prelu107"
  type: "PReLU"
  bottom: "conv107"
  top: "prelu107"
}
layer {
  name: "conv108"
  type: "Convolution"
  bottom: "prelu107"
  top: "conv108"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 64
    stride: 1
    dilation: 1
  }
}
layer {
  name: "conv109"
  type: "Convolution"
  bottom: "prelu107"
  top: "conv109"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 64
    stride: 1
    dilation: 1
  }
}
layer {
  name: "conv110"
  type: "Convolution"
  bottom: "prelu107"
  top: "conv110"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 2
    kernel_size: 3
    group: 64
    stride: 1
    dilation: 2
  }
}
layer {
  name: "conv111"
  type: "Convolution"
  bottom: "prelu107"
  top: "conv111"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 3
    kernel_size: 3
    group: 64
    stride: 1
    dilation: 3
  }
}
layer {
  name: "add112"
  type: "Eltwise"
  bottom: "conv108"
  bottom: "conv109"
  top: "add112"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "add113"
  type: "Eltwise"
  bottom: "add112"
  bottom: "conv110"
  top: "add113"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "add114"
  type: "Eltwise"
  bottom: "add113"
  bottom: "conv111"
  top: "add114"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "cat115"
  type: "Concat"
  bottom: "conv108"
  bottom: "add112"
  bottom: "add113"
  bottom: "add114"
  top: "cat115"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bn115"
  type: "BatchNorm"
  bottom: "cat115"
  top: "bn115"
}
layer {
  name: "prelu115"
  type: "PReLU"
  bottom: "bn115"
  top: "prelu115"
}
layer {
  name: "conv116"
  type: "Convolution"
  bottom: "prelu115"
  top: "conv116"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    group: 4
    stride: 1
  }
}
layer {
  name: "add117"
  type: "Eltwise"
  bottom: "prelu106"
  bottom: "conv116"
  top: "add117"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "prelu117"
  type: "PReLU"
  bottom: "add117"
  top: "prelu117"
}
layer {
  name: "conv118"
  type: "Convolution"
  bottom: "prelu117"
  top: "conv118"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    kernel_size: 1
    group: 4
    stride: 1
  }
}
layer {
  name: "prelu118"
  type: "PReLU"
  bottom: "conv118"
  top: "prelu118"
}
layer {
  name: "conv119"
  type: "Convolution"
  bottom: "prelu118"
  top: "conv119"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 64
    stride: 1
    dilation: 1
  }
}
layer {
  name: "conv120"
  type: "Convolution"
  bottom: "prelu118"
  top: "conv120"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 64
    stride: 1
    dilation: 1
  }
}
layer {
  name: "conv121"
  type: "Convolution"
  bottom: "prelu118"
  top: "conv121"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 2
    kernel_size: 3
    group: 64
    stride: 1
    dilation: 2
  }
}
layer {
  name: "conv122"
  type: "Convolution"
  bottom: "prelu118"
  top: "conv122"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 3
    kernel_size: 3
    group: 64
    stride: 1
    dilation: 3
  }
}
layer {
  name: "add123"
  type: "Eltwise"
  bottom: "conv119"
  bottom: "conv120"
  top: "add123"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "add124"
  type: "Eltwise"
  bottom: "add123"
  bottom: "conv121"
  top: "add124"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "add125"
  type: "Eltwise"
  bottom: "add124"
  bottom: "conv122"
  top: "add125"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "cat126"
  type: "Concat"
  bottom: "conv119"
  bottom: "add123"
  bottom: "add124"
  bottom: "add125"
  top: "cat126"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bn126"
  type: "BatchNorm"
  bottom: "cat126"
  top: "bn126"
}
layer {
  name: "prelu126"
  type: "PReLU"
  bottom: "bn126"
  top: "prelu126"
}
layer {
  name: "conv127"
  type: "Convolution"
  bottom: "prelu126"
  top: "conv127"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    group: 4
    stride: 1
  }
}
layer {
  name: "add128"
  type: "Eltwise"
  bottom: "prelu117"
  bottom: "conv127"
  top: "add128"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "prelu128"
  type: "PReLU"
  bottom: "add128"
  top: "prelu128"
}
layer {
  name: "conv129"
  type: "Convolution"
  bottom: "prelu128"
  top: "conv129"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    kernel_size: 1
    group: 4
    stride: 1
  }
}
layer {
  name: "prelu129"
  type: "PReLU"
  bottom: "conv129"
  top: "prelu129"
}
layer {
  name: "conv130"
  type: "Convolution"
  bottom: "prelu129"
  top: "conv130"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 64
    stride: 1
    dilation: 1
  }
}
layer {
  name: "conv131"
  type: "Convolution"
  bottom: "prelu129"
  top: "conv131"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 64
    stride: 1
    dilation: 1
  }
}
layer {
  name: "conv132"
  type: "Convolution"
  bottom: "prelu129"
  top: "conv132"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 2
    kernel_size: 3
    group: 64
    stride: 1
    dilation: 2
  }
}
layer {
  name: "conv133"
  type: "Convolution"
  bottom: "prelu129"
  top: "conv133"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 3
    kernel_size: 3
    group: 64
    stride: 1
    dilation: 3
  }
}
layer {
  name: "add134"
  type: "Eltwise"
  bottom: "conv130"
  bottom: "conv131"
  top: "add134"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "add135"
  type: "Eltwise"
  bottom: "add134"
  bottom: "conv132"
  top: "add135"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "add136"
  type: "Eltwise"
  bottom: "add135"
  bottom: "conv133"
  top: "add136"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "cat137"
  type: "Concat"
  bottom: "conv130"
  bottom: "add134"
  bottom: "add135"
  bottom: "add136"
  top: "cat137"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bn137"
  type: "BatchNorm"
  bottom: "cat137"
  top: "bn137"
}
layer {
  name: "prelu137"
  type: "PReLU"
  bottom: "bn137"
  top: "prelu137"
}
layer {
  name: "conv138"
  type: "Convolution"
  bottom: "prelu137"
  top: "conv138"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    group: 4
    stride: 1
  }
}
layer {
  name: "add139"
  type: "Eltwise"
  bottom: "prelu128"
  bottom: "conv138"
  top: "add139"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "prelu139"
  type: "PReLU"
  bottom: "add139"
  top: "prelu139"
}
layer {
  name: "conv140"
  type: "Convolution"
  bottom: "prelu139"
  top: "conv140"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    kernel_size: 1
    group: 4
    stride: 1
  }
}
layer {
  name: "prelu140"
  type: "PReLU"
  bottom: "conv140"
  top: "prelu140"
}
layer {
  name: "conv141"
  type: "Convolution"
  bottom: "prelu140"
  top: "conv141"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 64
    stride: 1
    dilation: 1
  }
}
layer {
  name: "conv142"
  type: "Convolution"
  bottom: "prelu140"
  top: "conv142"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 64
    stride: 1
    dilation: 1
  }
}
layer {
  name: "conv143"
  type: "Convolution"
  bottom: "prelu140"
  top: "conv143"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 2
    kernel_size: 3
    group: 64
    stride: 1
    dilation: 2
  }
}
layer {
  name: "conv144"
  type: "Convolution"
  bottom: "prelu140"
  top: "conv144"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 3
    kernel_size: 3
    group: 64
    stride: 1
    dilation: 3
  }
}
layer {
  name: "add145"
  type: "Eltwise"
  bottom: "conv141"
  bottom: "conv142"
  top: "add145"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "add146"
  type: "Eltwise"
  bottom: "add145"
  bottom: "conv143"
  top: "add146"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "add147"
  type: "Eltwise"
  bottom: "add146"
  bottom: "conv144"
  top: "add147"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "cat148"
  type: "Concat"
  bottom: "conv141"
  bottom: "add145"
  bottom: "add146"
  bottom: "add147"
  top: "cat148"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bn148"
  type: "BatchNorm"
  bottom: "cat148"
  top: "bn148"
}
layer {
  name: "prelu148"
  type: "PReLU"
  bottom: "bn148"
  top: "prelu148"
}
layer {
  name: "conv149"
  type: "Convolution"
  bottom: "prelu148"
  top: "conv149"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    group: 4
    stride: 1
  }
}
layer {
  name: "add150"
  type: "Eltwise"
  bottom: "prelu139"
  bottom: "conv149"
  top: "add150"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "prelu150"
  type: "PReLU"
  bottom: "add150"
  top: "prelu150"
}
layer {
  name: "conv151"
  type: "Convolution"
  bottom: "prelu150"
  top: "conv151"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    kernel_size: 1
    group: 4
    stride: 1
  }
}
layer {
  name: "prelu151"
  type: "PReLU"
  bottom: "conv151"
  top: "prelu151"
}
layer {
  name: "conv152"
  type: "Convolution"
  bottom: "prelu151"
  top: "conv152"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 64
    stride: 1
    dilation: 1
  }
}
layer {
  name: "conv153"
  type: "Convolution"
  bottom: "prelu151"
  top: "conv153"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 64
    stride: 1
    dilation: 1
  }
}
layer {
  name: "conv154"
  type: "Convolution"
  bottom: "prelu151"
  top: "conv154"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 2
    kernel_size: 3
    group: 64
    stride: 1
    dilation: 2
  }
}
layer {
  name: "conv155"
  type: "Convolution"
  bottom: "prelu151"
  top: "conv155"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 3
    kernel_size: 3
    group: 64
    stride: 1
    dilation: 3
  }
}
layer {
  name: "add156"
  type: "Eltwise"
  bottom: "conv152"
  bottom: "conv153"
  top: "add156"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "add157"
  type: "Eltwise"
  bottom: "add156"
  bottom: "conv154"
  top: "add157"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "add158"
  type: "Eltwise"
  bottom: "add157"
  bottom: "conv155"
  top: "add158"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "cat159"
  type: "Concat"
  bottom: "conv152"
  bottom: "add156"
  bottom: "add157"
  bottom: "add158"
  top: "cat159"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bn159"
  type: "BatchNorm"
  bottom: "cat159"
  top: "bn159"
}
layer {
  name: "prelu159"
  type: "PReLU"
  bottom: "bn159"
  top: "prelu159"
}
layer {
  name: "conv160"
  type: "Convolution"
  bottom: "prelu159"
  top: "conv160"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    group: 4
    stride: 1
  }
}
layer {
  name: "add161"
  type: "Eltwise"
  bottom: "prelu150"
  bottom: "conv160"
  top: "add161"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "prelu161"
  type: "PReLU"
  bottom: "add161"
  top: "prelu161"
}
layer {
  name: "conv162"
  type: "Convolution"
  bottom: "prelu161"
  top: "conv162"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    kernel_size: 1
    group: 4
    stride: 1
  }
}
layer {
  name: "prelu162"
  type: "PReLU"
  bottom: "conv162"
  top: "prelu162"
}
layer {
  name: "conv163"
  type: "Convolution"
  bottom: "prelu162"
  top: "conv163"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 64
    stride: 1
    dilation: 1
  }
}
layer {
  name: "conv164"
  type: "Convolution"
  bottom: "prelu162"
  top: "conv164"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 64
    stride: 1
    dilation: 1
  }
}
layer {
  name: "conv165"
  type: "Convolution"
  bottom: "prelu162"
  top: "conv165"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 2
    kernel_size: 3
    group: 64
    stride: 1
    dilation: 2
  }
}
layer {
  name: "conv166"
  type: "Convolution"
  bottom: "prelu162"
  top: "conv166"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 3
    kernel_size: 3
    group: 64
    stride: 1
    dilation: 3
  }
}
layer {
  name: "add167"
  type: "Eltwise"
  bottom: "conv163"
  bottom: "conv164"
  top: "add167"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "add168"
  type: "Eltwise"
  bottom: "add167"
  bottom: "conv165"
  top: "add168"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "add169"
  type: "Eltwise"
  bottom: "add168"
  bottom: "conv166"
  top: "add169"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "cat170"
  type: "Concat"
  bottom: "conv163"
  bottom: "add167"
  bottom: "add168"
  bottom: "add169"
  top: "cat170"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bn170"
  type: "BatchNorm"
  bottom: "cat170"
  top: "bn170"
}
layer {
  name: "prelu170"
  type: "PReLU"
  bottom: "bn170"
  top: "prelu170"
}
layer {
  name: "conv171"
  type: "Convolution"
  bottom: "prelu170"
  top: "conv171"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    group: 4
    stride: 1
  }
}
layer {
  name: "add172"
  type: "Eltwise"
  bottom: "prelu161"
  bottom: "conv171"
  top: "add172"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "prelu172"
  type: "PReLU"
  bottom: "add172"
  top: "prelu172"
}
layer {
  name: "conv173"
  type: "Convolution"
  bottom: "prelu172"
  top: "conv173"
  convolution_param {
    num_output: 10
    bias_term: false
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: "prelu173"
  type: "PReLU"
  bottom: "conv173"
  top: "prelu173"
}
layer {
  name: "deconv174"
  type: "Deconvolution"
  bottom: "prelu173"
  top: "deconv174"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 10
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 10
    stride: 2
    dilation: 1
  }
}
layer {
  name: "conv175"
  type: "Convolution"
  bottom: "deconv174"
  top: "conv175"
  convolution_param {
    num_output: 10
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 10
    stride: 1
  }
}
layer {
  name: "pool176"
  type: "Pooling"
  bottom: "conv175"
  top: "pool176"
  pooling_param {
    pool: AVE
    kernel_size: 4
    stride: 2
    pad: 1
  }
}
layer {
  name: "conv177"
  type: "Convolution"
  bottom: "prelu173"
  top: "conv177"
  convolution_param {
    num_output: 10
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 10
    stride: 1
  }
}
layer {
  name: "pool178"
  type: "Pooling"
  bottom: "prelu173"
  top: "pool178"
  pooling_param {
    pool: AVE
    kernel_size: 4
    stride: 2
    pad: 1
  }
}
layer {
  name: "conv179"
  type: "Convolution"
  bottom: "pool178"
  top: "conv179"
  convolution_param {
    num_output: 10
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 10
    stride: 1
  }
}
layer {
  name: "deconv180"
  type: "Deconvolution"
  bottom: "conv179"
  top: "deconv180"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 10
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 10
    stride: 2
    dilation: 1
  }
}
layer {
  name: "pool181"
  type: "Pooling"
  bottom: "prelu173"
  top: "pool181"
  pooling_param {
    pool: AVE
    kernel_size: 12
    stride: 10
    pad: 1
  }
}
layer {
  name: "conv182"
  type: "Convolution"
  bottom: "pool181"
  top: "conv182"
  convolution_param {
    num_output: 10
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 10
    stride: 1
  }
}
layer {
  name: "deconv183"
  type: "Deconvolution"
  bottom: "conv182"
  top: "deconv183"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 10
    bias_term: false
    pad: 1
    kernel_size: 12
    group: 10
    stride: 10
    dilation: 1
  }
}
layer {
  name: "cat184"
  type: "Concat"
  bottom: "pool176"
  bottom: "conv177"
  bottom: "deconv180"
  bottom: "deconv183"
  top: "cat184"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bn185"
  type: "BatchNorm"
  bottom: "cat184"
  top: "bn185"
}
layer {
  name: "prelu185"
  type: "PReLU"
  bottom: "bn185"
  top: "prelu185"
}
layer {
  name: "conv186"
  type: "Convolution"
  bottom: "prelu185"
  top: "conv186"
  convolution_param {
    num_output: 10
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 10
    stride: 1
  }
}
layer {
  name: "prelu186"
  type: "PReLU"
  bottom: "conv186"
  top: "prelu186"
}
layer {
  name: "conv187"
  type: "Convolution"
  bottom: "prelu186"
  top: "conv187"
  convolution_param {
    num_output: 64
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn188"
  type: "BatchNorm"
  bottom: "conv187"
  top: "bn188"
}
layer {
  name: "prelu188"
  type: "PReLU"
  bottom: "bn188"
  top: "prelu188"
}
layer {
  name: "deconv189"
  type: "Deconvolution"
  bottom: "prelu188"
  top: "deconv189"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    stride: 2
    dilation: 1
  }
}
layer {
  name: "conv190"
  type: "Convolution"
  bottom: "prelu68"
  top: "conv190"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 64
    stride: 1
  }
}
layer {
  name: "prelu190"
  type: "PReLU"
  bottom: "conv190"
  top: "prelu190"
}
layer {
  name: "add191"
  type: "Eltwise"
  bottom: "prelu190"
  bottom: "deconv189"
  top: "add191"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "bn192"
  type: "BatchNorm"
  bottom: "add191"
  top: "bn192"
}
layer {
  name: "prelu192"
  type: "PReLU"
  bottom: "bn192"
  top: "prelu192"
}
layer {
  name: "conv193"
  type: "Convolution"
  bottom: "prelu192"
  top: "conv193"
  convolution_param {
    num_output: 10
    bias_term: false
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: "prelu193"
  type: "PReLU"
  bottom: "conv193"
  top: "prelu193"
}
layer {
  name: "deconv194"
  type: "Deconvolution"
  bottom: "prelu193"
  top: "deconv194"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 10
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 10
    stride: 2
    dilation: 1
  }
}
layer {
  name: "conv195"
  type: "Convolution"
  bottom: "deconv194"
  top: "conv195"
  convolution_param {
    num_output: 10
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 10
    stride: 1
  }
}
layer {
  name: "pool196"
  type: "Pooling"
  bottom: "conv195"
  top: "pool196"
  pooling_param {
    pool: AVE
    kernel_size: 4
    stride: 2
    pad: 1
  }
}
layer {
  name: "conv197"
  type: "Convolution"
  bottom: "prelu193"
  top: "conv197"
  convolution_param {
    num_output: 10
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 10
    stride: 1
  }
}
layer {
  name: "pool198"
  type: "Pooling"
  bottom: "prelu193"
  top: "pool198"
  pooling_param {
    pool: AVE
    kernel_size: 4
    stride: 2
    pad: 1
  }
}
layer {
  name: "conv199"
  type: "Convolution"
  bottom: "pool198"
  top: "conv199"
  convolution_param {
    num_output: 10
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 10
    stride: 1
  }
}
layer {
  name: "deconv200"
  type: "Deconvolution"
  bottom: "conv199"
  top: "deconv200"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 10
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 10
    stride: 2
    dilation: 1
  }
}
layer {
  name: "pool201"
  type: "Pooling"
  bottom: "prelu193"
  top: "pool201"
  pooling_param {
    pool: AVE
    kernel_size: 12
    stride: 10
    pad: 1
  }
}
layer {
  name: "conv202"
  type: "Convolution"
  bottom: "pool201"
  top: "conv202"
  convolution_param {
    num_output: 10
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 10
    stride: 1
  }
}
layer {
  name: "deconv203"
  type: "Deconvolution"
  bottom: "conv202"
  top: "deconv203"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 10
    bias_term: false
    pad: 1
    kernel_size: 12
    group: 10
    stride: 10
    dilation: 1
  }
}
layer {
  name: "cat204"
  type: "Concat"
  bottom: "pool196"
  bottom: "conv197"
  bottom: "deconv200"
  bottom: "deconv203"
  top: "cat204"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bn205"
  type: "BatchNorm"
  bottom: "cat204"
  top: "bn205"
}
layer {
  name: "prelu205"
  type: "PReLU"
  bottom: "bn205"
  top: "prelu205"
}
layer {
  name: "conv206"
  type: "Convolution"
  bottom: "prelu205"
  top: "conv206"
  convolution_param {
    num_output: 10
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 10
    stride: 1
  }
}
layer {
  name: "prelu206"
  type: "PReLU"
  bottom: "conv206"
  top: "prelu206"
}
layer {
  name: "conv207"
  type: "Convolution"
  bottom: "prelu206"
  top: "conv207"
  convolution_param {
    num_output: 48
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn208"
  type: "BatchNorm"
  bottom: "conv207"
  top: "bn208"
}
layer {
  name: "prelu208"
  type: "PReLU"
  bottom: "bn208"
  top: "prelu208"
}
layer {
  name: "deconv209"
  type: "Deconvolution"
  bottom: "prelu208"
  top: "deconv209"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 4
    stride: 2
    dilation: 1
  }
}
layer {
  name: "conv210"
  type: "Convolution"
  bottom: "prelu17"
  top: "conv210"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 16
    stride: 1
  }
}
layer {
  name: "prelu210"
  type: "PReLU"
  bottom: "conv210"
  top: "prelu210"
}
layer {
  name: "add211"
  type: "Eltwise"
  bottom: "prelu210"
  bottom: "deconv209"
  top: "add211"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "bn212"
  type: "BatchNorm"
  bottom: "add211"
  top: "bn212"
}
layer {
  name: "prelu212"
  type: "PReLU"
  bottom: "bn212"
  top: "prelu212"
}
layer {
  name: "conv213"
  type: "Convolution"
  bottom: "prelu212"
  top: "conv213"
  convolution_param {
    num_output: 10
    bias_term: false
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: "prelu213"
  type: "PReLU"
  bottom: "conv213"
  top: "prelu213"
}
layer {
  name: "deconv214"
  type: "Deconvolution"
  bottom: "prelu213"
  top: "deconv214"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 10
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 10
    stride: 2
    dilation: 1
  }
}
layer {
  name: "conv215"
  type: "Convolution"
  bottom: "deconv214"
  top: "conv215"
  convolution_param {
    num_output: 10
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 10
    stride: 1
  }
}
layer {
  name: "pool216"
  type: "Pooling"
  bottom: "conv215"
  top: "pool216"
  pooling_param {
    pool: AVE
    kernel_size: 4
    stride: 2
    pad: 1
  }
}
layer {
  name: "conv217"
  type: "Convolution"
  bottom: "prelu213"
  top: "conv217"
  convolution_param {
    num_output: 10
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 10
    stride: 1
  }
}
layer {
  name: "pool218"
  type: "Pooling"
  bottom: "prelu213"
  top: "pool218"
  pooling_param {
    pool: AVE
    kernel_size: 4
    stride: 2
    pad: 1
  }
}
layer {
  name: "conv219"
  type: "Convolution"
  bottom: "pool218"
  top: "conv219"
  convolution_param {
    num_output: 10
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 10
    stride: 1
  }
}
layer {
  name: "deconv220"
  type: "Deconvolution"
  bottom: "conv219"
  top: "deconv220"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 10
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 10
    stride: 2
    dilation: 1
  }
}
layer {
  name: "pool221"
  type: "Pooling"
  bottom: "prelu213"
  top: "pool221"
  pooling_param {
    pool: AVE
    kernel_size: 12
    stride: 10
    pad: 1
  }
}
layer {
  name: "conv222"
  type: "Convolution"
  bottom: "pool221"
  top: "conv222"
  convolution_param {
    num_output: 10
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 10
    stride: 1
  }
}
layer {
  name: "deconv223"
  type: "Deconvolution"
  bottom: "conv222"
  top: "deconv223"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 10
    bias_term: false
    pad: 1
    kernel_size: 12
    group: 10
    stride: 10
    dilation: 1
  }
}
layer {
  name: "cat224"
  type: "Concat"
  bottom: "pool216"
  bottom: "conv217"
  bottom: "deconv220"
  bottom: "deconv223"
  top: "cat224"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bn225"
  type: "BatchNorm"
  bottom: "cat224"
  top: "bn225"
}
layer {
  name: "prelu225"
  type: "PReLU"
  bottom: "bn225"
  top: "prelu225"
}
layer {
  name: "conv226"
  type: "Convolution"
  bottom: "prelu225"
  top: "conv226"
  convolution_param {
    num_output: 10
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 10
    stride: 1
  }
}
layer {
  name: "prelu226"
  type: "PReLU"
  bottom: "conv226"
  top: "prelu226"
}
layer {
  name: "conv227"
  type: "Convolution"
  bottom: "prelu226"
  top: "conv227"
  convolution_param {
    num_output: 32
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn228"
  type: "BatchNorm"
  bottom: "conv227"
  top: "bn228"
}
layer {
  name: "prelu228"
  type: "PReLU"
  bottom: "bn228"
  top: "prelu228"
}
layer {
  name: "deconv229"
  type: "Deconvolution"
  bottom: "prelu228"
  top: "deconv229"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 4
    stride: 2
    dilation: 1
  }
}
layer {
  name: "conv230"
  type: "Convolution"
  bottom: "prelu0"
  top: "conv230"
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 32
    stride: 1
  }
}
layer {
  name: "prelu230"
  type: "PReLU"
  bottom: "conv230"
  top: "prelu230"
}
layer {
  name: "add231"
  type: "Eltwise"
  bottom: "prelu230"
  bottom: "deconv229"
  top: "add231"
  eltwise_param {
    operation: SUM
    coeff: 1.0
    coeff: 1.0
  }
}
layer {
  name: "bn232"
  type: "BatchNorm"
  bottom: "add231"
  top: "bn232"
}
layer {
  name: "prelu232"
  type: "PReLU"
  bottom: "bn232"
  top: "prelu232"
}
layer {
  name: "conv233"
  type: "Convolution"
  bottom: "prelu232"
  top: "conv233"
  convolution_param {
    num_output: 10
    bias_term: false
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: "prelu233"
  type: "PReLU"
  bottom: "conv233"
  top: "prelu233"
}
layer {
  name: "deconv234"
  type: "Deconvolution"
  bottom: "prelu233"
  top: "deconv234"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 10
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 10
    stride: 2
    dilation: 1
  }
}
layer {
  name: "conv235"
  type: "Convolution"
  bottom: "deconv234"
  top: "conv235"
  convolution_param {
    num_output: 10
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 10
    stride: 1
  }
}
layer {
  name: "pool236"
  type: "Pooling"
  bottom: "conv235"
  top: "pool236"
  pooling_param {
    pool: AVE
    kernel_size: 4
    stride: 2
    pad: 1
  }
}
layer {
  name: "conv237"
  type: "Convolution"
  bottom: "prelu233"
  top: "conv237"
  convolution_param {
    num_output: 10
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 10
    stride: 1
  }
}
layer {
  name: "pool238"
  type: "Pooling"
  bottom: "prelu233"
  top: "pool238"
  pooling_param {
    pool: AVE
    kernel_size: 4
    stride: 2
    pad: 1
  }
}
layer {
  name: "conv239"
  type: "Convolution"
  bottom: "pool238"
  top: "conv239"
  convolution_param {
    num_output: 10
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 10
    stride: 1
  }
}
layer {
  name: "deconv240"
  type: "Deconvolution"
  bottom: "conv239"
  top: "deconv240"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 10
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 10
    stride: 2
    dilation: 1
  }
}
layer {
  name: "pool241"
  type: "Pooling"
  bottom: "prelu233"
  top: "pool241"
  pooling_param {
    pool: AVE
    kernel_size: 12
    stride: 10
    pad: 1
  }
}
layer {
  name: "conv242"
  type: "Convolution"
  bottom: "pool241"
  top: "conv242"
  convolution_param {
    num_output: 10
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 10
    stride: 1
  }
}
layer {
  name: "deconv243"
  type: "Deconvolution"
  bottom: "conv242"
  top: "deconv243"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 10
    bias_term: false
    pad: 1
    kernel_size: 12
    group: 10
    stride: 10
    dilation: 1
  }
}
layer {
  name: "cat244"
  type: "Concat"
  bottom: "pool236"
  bottom: "conv237"
  bottom: "deconv240"
  bottom: "deconv243"
  top: "cat244"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bn245"
  type: "BatchNorm"
  bottom: "cat244"
  top: "bn245"
}
layer {
  name: "prelu245"
  type: "PReLU"
  bottom: "bn245"
  top: "prelu245"
}
layer {
  name: "conv246"
  type: "Convolution"
  bottom: "prelu245"
  top: "conv246"
  convolution_param {
    num_output: 10
    bias_term: false
    pad: 1
    kernel_size: 3
    group: 10
    stride: 1
  }
}
layer {
  name: "prelu246"
  type: "PReLU"
  bottom: "conv246"
  top: "prelu246"
}
layer {
  name: "conv247"
  type: "Convolution"
  bottom: "prelu246"
  top: "conv247"
  convolution_param {
    num_output: 20
    bias_term: false
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn248"
  type: "BatchNorm"
  bottom: "conv247"
  top: "bn248"
}
layer {
  name: "prelu248"
  type: "PReLU"
  bottom: "bn248"
  top: "prelu248"
}
